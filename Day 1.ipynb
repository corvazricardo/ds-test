{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 1\n",
    "- Reference: [1.Data Preprocessing](https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Code/Day%201_Data%20PreProcessing.md)\n",
    "\n",
    "Notebook to implement the basic 6 steps of Data Preprocessing, as explained in **1.Data Preprocessing**\n",
    "\n",
    "### <span style='color:green'>Steps</span>\n",
    "\n",
    "\n",
    "- <span style='color:green'>**1)Importing Required Libraries**</span>: Such as **Numpy** (contains Mathematical functions) and Pandas (imports and manages datasets).\n",
    "\n",
    "- <span style='color:green'>**2)Importing Dataset**</span>: Mainly imported as a Dataframe. Then we make separate Matrix and Vector of Independent and Dependent variables from the DF. Independent Variables are the \"Features\" for the process under analysis, whilst Dependent Variables are usually the \"Output\". See [Independent and Dependent variables on ML](https://www.dailysmarty.com/posts/difference-between-independent-and-dependent-variables-in-machine-learning)) and [What is an Independent and a Dependent variable?](https://www.scribbr.com/methodology/independent-and-dependent-variables/).\n",
    "\n",
    "- <span style='color:green'>**3)Handling the Missing Data**</span>: Data is rarely homogeneous. Replacing missing data by the Mean or Median of the entire column is typically done. It helps on avoiding any reduction of performance of the ML model. We can use `SimpleImputer` class of `sklearn.impute` for this step.\n",
    "\n",
    "- <span style='color:green'>**4)Encoding Categorical Data**</span>: Categorical variables contain label values instead of numerical values. The number of possible values is often limited to a fixed set. Example values can be `Yes`/`No`, `Male`/`Female` which *cannot be used in mathematical equations* so we encode these variables into numbers. The `Label Encoder` class from `sklearn.preprocessing` helps on this task.\n",
    "\n",
    "- <span style='color:green'>**5)Feature Scaling**</span>: Refers to putting the values in the same range or scale so that no variable is dominated by other. Most of ML algorights (*KNNs*, *PCA*, *LDA*,etc) use the Euclidean distance between 2 data points in their computations. Features highly varying in magnitudes, units and range pose problems. **Standardization** or **Z-score normalization** involves rescaling the features such that they have the properties of a standard normal distribution with a mean of zero and a standard deviation of one. We import `StandardScaler` from `sklearn.preprocessing`. See [Importance of Feature Scaling](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html) and [Why Feature Scaling is required?](https://medium.com/@rahul77349/feature-scaling-why-it-is-required-8a93df1af310).\n",
    "\n",
    "- <span style='color:green'>**6)Splitting Dataset into test set and training set**</span>: We make 2 partitions of dataset, one for training the model:called *Training Set*, and one for testing the performance of the trained model: called *Test Set*. The split is generally 80/20. We can import `train_test_split` method of `sklearn.model_selection ` library.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Keywords: Import libraries, import dataset, handle missing data, encoding categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:green'>**Step 1:Importing Required Libraries**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:green'>**Step 2:Importing Dataset**</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Independent Variables:\n",
      "[['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]]\n",
      "Dependent Variables:\n",
      "['Yes' 'No']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"datasets/Data.csv\")\n",
    "#Selecting Independent Variables\n",
    "X = df.iloc[ : , :-1].values\n",
    "print(\"Independent Variables:\\n{}\".format(X[1:3,:]))\n",
    "#Selecting Dependent Variables\n",
    "Y = df.iloc[ : , 3].values\n",
    "print(\"Dependent Variables:\\n{}\".format(Y[1:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:green'>**Step 3: Handling the missing data**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer \n",
    "imputer = SimpleImputer(missing_values = np.nan, strategy = \"mean\")\n",
    "imputer = imputer.fit(X[ : , 1:])\n",
    "X[ : , 1:3] = imputer.transform(X[ : , 1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean of variable replaces entries with missing data \n",
    "\n",
    "`X` **before transform** `X[ : , 1:3] = imputer.transform(X[ : , 1:])` is: \n",
    "       \n",
    "      `array([['France', 44.0, 72000.0],\n",
    "       ['Spain', 27.0, 48000.0],\n",
    "       ['Germany', 30.0, 54000.0],\n",
    "       ['Spain', 38.0, 61000.0],\n",
    "       ['Germany', 40.0, nan],\n",
    "       ['France', 35.0, 58000.0],\n",
    "       ['Spain', nan, 52000.0],\n",
    "       ['France', 48.0, 79000.0],\n",
    "       ['Germany', 50.0, 83000.0],\n",
    "       ['France', 37.0, 67000.0]], dtype=object)`\n",
    "       \n",
    "`X` **after transform** is:\n",
    "\n",
    "       `array([['France', 44.0, 72000.0],\n",
    "       ['Spain', 27.0, 48000.0],\n",
    "       ['Germany', 30.0, 54000.0],\n",
    "       ['Spain', 38.0, 61000.0],\n",
    "       ['Germany', 40.0, 63777.77777777778],\n",
    "       ['France', 35.0, 58000.0],\n",
    "       ['Spain', 38.77777777777778, 52000.0],\n",
    "       ['France', 48.0, 79000.0],\n",
    "       ['Germany', 50.0, 83000.0],\n",
    "       ['France', 37.0, 67000.0]], dtype=object)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:green'>**Step 4:Encoding Categorical Data**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X = LabelEncoder()\n",
    "#creating copy to see result of encode\n",
    "X_copy = X[:]\n",
    "X_copy[ : , 0] = labelencoder_X.fit_transform(X_copy[ : , 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now categorical values are represented by a numerical value. *Categorical values start from 0 and goes all the way up to N-1 categories*\n",
    "\n",
    "`X` **before Encoding** `X[ : , 0] = labelencoder_X.fit_transform(X[ : , 0])` is: \n",
    "       \n",
    "      array([['France', 44.0, 72000.0],\n",
    "       ['Spain', 27.0, 48000.0],\n",
    "       ['Germany', 30.0, 54000.0],\n",
    "       ['Spain', 38.0, 61000.0],\n",
    "       ['Germany', 40.0, nan],\n",
    "       ['France', 35.0, 58000.0],\n",
    "       ['Spain', nan, 52000.0],\n",
    "       ['France', 48.0, 79000.0],\n",
    "       ['Germany', 50.0, 83000.0],\n",
    "       ['France', 37.0, 67000.0]], dtype=object)\n",
    "\n",
    "`X` **after transform** is:\n",
    "\n",
    "    array([[0, 44.0, 72000.0],\n",
    "       [2, 27.0, 48000.0],\n",
    "       [1, 30.0, 54000.0],\n",
    "       [2, 38.0, 61000.0],\n",
    "       [1, 40.0, 63777.77777777778],\n",
    "       [0, 35.0, 58000.0],\n",
    "       [2, 38.77777777777778, 52000.0],\n",
    "       [0, 48.0, 79000.0],\n",
    "       [1, 50.0, 83000.0],\n",
    "       [0, 37.0, 67000.0]], dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement OneHoEncoding**\n",
    "\n",
    "Label Encoding is not sufficient to provide the model for training. It assumes the higher the categorical value, the better the category. See [Why Label Enconding is not sufficient?](https://medium.com/hackernoon/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f#:~:text=One%20hot%20encoding%20is%20a,a%20better%20job%20in%20prediction.&text=The%20categorical%20value%20represents%20the,the%20entry%20in%20the%20dataset.)\n",
    "\n",
    "Using OneHotEncoder helps to perform **binarization** of the category and include it as a feature for training.\n",
    "\n",
    "Tip: `LabelEncoder` can be ommited and use `OneHotEncoder` directly. See [How to implement OneHotEncoder?](https://stackoverflow.com/questions/54345667/onehotencoder-categorical-features-depreciated-how-to-transform-specific-column) and [Get feature names after OneHotEncode In ColumnTransformer](https://stackoverflow.com/questions/54646709/sklearn-pipeline-get-feature-names-after-onehotencode-in-columntransformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cortezvazquezo/opt/anaconda3/envs/coolart37/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Encode Country Column\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "#Country is just the name of the step. \n",
    "#You can name it as you want, and it can be useful to call this step in the future, \n",
    "#for example if you just need to set/get the parameter of one step\n",
    "ct = ColumnTransformer([(\"Country\", OneHotEncoder(), [0])], remainder = 'passthrough')\n",
    "X = ct.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`X` after **OneHotEncoding** has a binary feature per category in the desired variable:\n",
    "\n",
    "       array([[1.0, 0.0, 0.0, 44.0, 72000.0],\n",
    "       [0.0, 0.0, 1.0, 27.0, 48000.0],\n",
    "       [0.0, 1.0, 0.0, 30.0, 54000.0],\n",
    "       [0.0, 0.0, 1.0, 38.0, 61000.0],\n",
    "       [0.0, 1.0, 0.0, 40.0, 63777.77777777778],\n",
    "       [1.0, 0.0, 0.0, 35.0, 58000.0],\n",
    "       [0.0, 0.0, 1.0, 38.77777777777778, 52000.0],\n",
    "       [1.0, 0.0, 0.0, 48.0, 79000.0],\n",
    "       [0.0, 1.0, 0.0, 50.0, 83000.0],\n",
    "       [1.0, 0.0, 0.0, 37.0, 67000.0]], dtype=object)\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:green'>**Step 5:Feature Scaling**</span>\n",
    "\n",
    "Feature scaling is not required on `Binary` or `Categorical` features, only on `Numerical` variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X[:,3:] = sc_X.fit_transform(X[:,3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`X` before **Feature Scaling**:\n",
    "\n",
    "       array([[1.0, 0.0, 0.0, 44.0, 72000.0],\n",
    "       [0.0, 0.0, 1.0, 27.0, 48000.0],\n",
    "       [0.0, 1.0, 0.0, 30.0, 54000.0],\n",
    "       [0.0, 0.0, 1.0, 38.0, 61000.0],\n",
    "       [0.0, 1.0, 0.0, 40.0, 63777.77777777778],\n",
    "       [1.0, 0.0, 0.0, 35.0, 58000.0],\n",
    "       [0.0, 0.0, 1.0, 38.77777777777778, 52000.0],\n",
    "       [1.0, 0.0, 0.0, 48.0, 79000.0],\n",
    "       [0.0, 1.0, 0.0, 50.0, 83000.0],\n",
    "       [1.0, 0.0, 0.0, 37.0, 67000.0]], dtype=object)\n",
    "       \n",
    "`X` after **Feature Scaling**: \n",
    "\n",
    "       array([[1.0, 0.0, 0.0, 0.758874361590019, 0.7494732544921677],\n",
    "       [0.0, 0.0, 1.0, -1.7115038793306814, -1.4381784072687531],\n",
    "       [0.0, 1.0, 0.0, -1.2755547779917342, -0.8912654918285229],\n",
    "       [0.0, 0.0, 1.0, -0.1130238410878753, -0.253200423814921],\n",
    "       [0.0, 1.0, 0.0, 0.17760889313808945, 6.632191985654332e-16],\n",
    "       [1.0, 0.0, 0.0, -0.5489729424268225, -0.5266568815350361],\n",
    "       [0.0, 0.0, 1.0, 0.0, -1.0735697969752662],\n",
    "       [1.0, 0.0, 0.0, 1.3401398300419485, 1.3875383225057696],\n",
    "       [0.0, 1.0, 0.0, 1.6307725642679132, 1.7521469327992565],\n",
    "       [1.0, 0.0, 0.0, -0.2583402082008577, 0.29371249162530916]],\n",
    "      dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='color:green'>**Step 6:Splitting Dataset into test set and training set**</span>\n",
    "\n",
    "Perform Train/Test Split together with Cross Validation do prevent (or at least minimize) **overfitting**. See [Overfitting,Underfitting and Cross-Validation](https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6)\n",
    "\n",
    "**Overfitting** is a model that learned too well the training data and is likely to not be accurate on predictions of new data. This happens when the model doesn't generalise because it has also learned the noise of the training data and won't be able to make any inferences on unseen data.\n",
    "\n",
    "**Underfitting** is the opposite to Overfitting. The model hasn't learned enough from the training data and therefore won't predict well on new data. This happens when the model doesn't fit the training data and consequently misses the trends in the data. Causing the model not to generalise well on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:(8, 5), X_test:(2, 5), Y_train:(8,), Y_test:(2,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split( X , Y , test_size = 0.2, random_state = 0)\n",
    "print(\"X_train:{}, X_test:{}, Y_train:{}, Y_test:{}\".format(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Samples of `X_train`:\n",
    "\n",
    "       array([[0.0, 1.0, 0.0, 0.17760889313808945, 6.632191985654332e-16],\n",
    "       [1.0, 0.0, 0.0, -0.2583402082008577, 0.29371249162530916],\n",
    "       [0.0, 0.0, 1.0, -1.7115038793306814, -1.4381784072687531],\n",
    "       [0.0, 0.0, 1.0, 0.0, -1.0735697969752662],\n",
    "       [1.0, 0.0, 0.0, 1.3401398300419485, 1.3875383225057696],\n",
    "       [0.0, 0.0, 1.0, -0.1130238410878753, -0.253200423814921],\n",
    "       [1.0, 0.0, 0.0, 0.758874361590019, 0.7494732544921677],\n",
    "       [1.0, 0.0, 0.0, -0.5489729424268225, -0.5266568815350361]],\n",
    "      dtype=object)\n",
    "      \n",
    "Samples of `X_test`:\n",
    "       \n",
    "       array([[0.0, 1.0, 0.0, -1.2755547779917342, -0.8912654918285229],\n",
    "       [0.0, 1.0, 0.0, 1.6307725642679132, 1.7521469327992565]],\n",
    "      dtype=object)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
